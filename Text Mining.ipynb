{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7742f2d5",
   "metadata": {},
   "source": [
    "# text mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70a4b18",
   "metadata": {},
   "source": [
    "You may conduct these exercises using just the first first 20,000 reviews from the corpus. Use the first half for training and the rest for testing. Process reviews without capitalization or punctuation (and without using stemming or removing stopwords)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfd5603",
   "metadata": {},
   "source": [
    "Q1. Build a sentiment analysis model that estimates star ratings from a 1,000 word bag-of-words model (based on the most popular words). \n",
    "\n",
    "Compare models based on:\n",
    "(a) the 1,000 most common unigrams;\n",
    "(b) the 1,000 most common bigrams;\n",
    "(c) a model which uses a combination of unigrams and bigrams (i.e., some bigrams will be included if they are more popular than some unigrams, but the model dimensionality will still be 1,000). \n",
    "\n",
    "You may use a Ridge regression model (sklearn.linear model.Ridge) with a regularization coefficient of Î» = 1). Report the MSE on the test set for each of the three variants, along with the five most positive and most negative tokens for each variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a52b384f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113928"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/Users/shiqi/Downloads/cse258/week 5-6 Text Mining/')\n",
    "\n",
    "from collections import defaultdict\n",
    "import string\n",
    "from sklearn import linear_model\n",
    "import math\n",
    "# using just the first first 20,000 reviews\n",
    "def parseData(fname):\n",
    "    for l in open(fname):\n",
    "        yield eval(l)\n",
    "data = list(parseData(\"goodreads_reviews_comics_graphic.json\"))[:20000]\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "for d in data:\n",
    "    for w in d['review_text'].split():\n",
    "        wordCount[w] += 1\n",
    "\n",
    "len(wordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff7a3465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59814"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process reviews without capitalization or punctuation\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in data:\n",
    "    r = ''.join([c for c in d['review_text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        wordCount[w] += 1\n",
    "\n",
    "len(wordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2282ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just build our feature vector by taking the most popular words (lowercase, punctuation removed)\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in data:\n",
    "  r = ''.join([c for c in d['review_text'].lower() if not c in punctuation])\n",
    "  for w in r.split():\n",
    "    wordCount[w] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b02d03cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1,000 word bag-of-words model (based on the most popular words)\n",
    "words = [x[1] for x in counts[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aec33700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 1,000 most common unigrams\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)\n",
    "\n",
    "def feature(datum):\n",
    "    feat = [0]*len(words)\n",
    "    r = ''.join([c for c in datum['review_text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if w in words:\n",
    "            feat[wordId[w]] += 1\n",
    "    feat.append(1) # offset\n",
    "    return feat\n",
    "\n",
    "y = [d['rating'] for d in data]\n",
    "y_train = y[:10000]\n",
    "y_test = y[10000:]\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "X_train = X[:10000]\n",
    "X_test = X[10000:]\n",
    "# Regularized regression\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False) # MSE + 1.0 l2\n",
    "clf.fit(X_train, y_train)\n",
    "theta = clf.coef_\n",
    "uni_gram_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb77ec52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram MSE =  1.2390553477075856\n"
     ]
    }
   ],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)\n",
    "\n",
    "print('unigram MSE = ',MSE(uni_gram_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00cec4a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.5297694170496591, 'boring'),\n",
       " (-0.5187275432483344, 'disappointing'),\n",
       " (-0.3449045061687771, 'says'),\n",
       " (-0.34445735816736345, 'worst'),\n",
       " (-0.3300903719725661, 'basically')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordSort = list(zip(theta[:-1], words))\n",
    "wordSort.sort()\n",
    "# Some of the most negative n-grams...\n",
    "wordSort[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eefb7528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.32142847836888355, '5'),\n",
       " (0.3429560361747083, 'yourself'),\n",
       " (0.370370980652106, 'beautifully'),\n",
       " (0.40761653981863377, 'mix'),\n",
       " (0.410004152269727, 'wait')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some of the most positive n-grams...\n",
    "wordSort[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb7b5da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 1,000 most common bigrams\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in data:\n",
    "    r = ''.join([c for c in d['review_text'].lower() if not c in punctuation])\n",
    "    ws = r.split()\n",
    "    ws2 = [' '.join(x) for x in list(zip(ws[:-1],ws[1:]))]\n",
    "    for w in ws2:\n",
    "        wordCount[w] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "words = [x[1] for x in counts[:1000]]\n",
    "\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)\n",
    "\n",
    "def feature(datum):\n",
    "    feat = [0]*len(words)\n",
    "    r = ''.join([c for c in datum['review_text'].lower() if not c in punctuation])\n",
    "    ws = r.split()\n",
    "    ws2 = [' '.join(x) for x in list(zip(ws[:-1],ws[1:]))]\n",
    "    for w in ws2:\n",
    "        if w in words:\n",
    "            feat[wordId[w]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "X_train = X[:10000]\n",
    "X_test = X[10000:]\n",
    "\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False) # MSE + 1.0 l2\n",
    "clf.fit(X_train, y_train)\n",
    "theta = clf.coef_\n",
    "bi_gram_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa663000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram MSE =  1.2930626118603752\n"
     ]
    }
   ],
   "source": [
    "print('bigram MSE = ',MSE(bi_gram_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7bf9a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-1.0181651480170788, 'tuned for'),\n",
       " (-0.6712583885760911, 'miss your'),\n",
       " (-0.6208746381941644, 'the worst'),\n",
       " (-0.5485595567492433, 'a bad'),\n",
       " (-0.5396115156910215, 'too many')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordSort = list(zip(theta[:-1], words))\n",
    "wordSort.sort()\n",
    "# Some of the most negative bigrams...\n",
    "wordSort[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "814e91b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.5647727696125389, 'reviews as'),\n",
       " (0.5670113632668727, '5 stars'),\n",
       " (0.6277280701070215, 'stay tuned'),\n",
       " (0.6362888029933468, 'cant wait'),\n",
       " (0.7930527784347808, 'forget to')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some of the most positive bigrams...\n",
    "wordSort[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6752d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 1,000 most common n_grams\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in data:\n",
    "    r = ''.join([c for c in d['review_text'].lower() if not c in punctuation])\n",
    "    ws = r.split()\n",
    "    ws2 = [' '.join(x) for x in list(zip(ws[:-1],ws[1:]))]\n",
    "    for w in ws + ws2:\n",
    "        wordCount[w] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "words = [x[1] for x in counts[:1000]]\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)\n",
    "\n",
    "def feature(datum):\n",
    "    feat = [0]*len(words)\n",
    "    r = ''.join([c for c in datum['review_text'].lower() if not c in punctuation])\n",
    "    ws = r.split()\n",
    "    ws2 = [' '.join(x) for x in list(zip(ws[:-1],ws[1:]))]\n",
    "    for w in ws + ws2:\n",
    "        if w in words:\n",
    "            feat[wordId[w]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "X_train = X[:10000]\n",
    "X_test = X[10000:]\n",
    "\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False) # MSE + 1.0 l2\n",
    "clf.fit(X_train, y_train)\n",
    "theta = clf.coef_\n",
    "n_gram_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "648417b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngrams MSE =  1.2366939869514855\n"
     ]
    }
   ],
   "source": [
    "# ngrams: a combination of unigrams and bigrams\n",
    "print('ngrams MSE = ',MSE(n_gram_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f459220a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.38054599248630927, 'katies corner'),\n",
       " (-0.3662044311971771, 'share'),\n",
       " (-0.34264460246037415, 'what is'),\n",
       " (-0.32910038400902614, 'least'),\n",
       " (-0.28446911724166346, 'able to')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordSort = list(zip(theta[:-1], words))\n",
    "wordSort.sort()\n",
    "# Some of the most negative n-grams...\n",
    "wordSort[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d754f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.31228135202807056, 'at least'),\n",
       " (0.35761224878132114, 'excellent'),\n",
       " (0.3695932391755788, 'wait'),\n",
       " (0.4933603984754807, 'able'),\n",
       " (0.5044389422876421, 'katies')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some of the most positive n-grams...\n",
    "wordSort[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f256a8a",
   "metadata": {},
   "source": [
    "Q2. Which review has the highest cosine similarity compared to the first review in the dataset, in terms of their tf-idf representations (using only the training set, and considering unigrams only)? Provide the ID and text of the review (2 marks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0afc328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2 TF_IDFs\n",
    "# using only the training set, and considering unigrams only\n",
    "train = data[:10000]\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in train:\n",
    "    r = ''.join([c for c in d['review_text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        wordCount[w] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "words = [x[1] for x in counts[:1000]]\n",
    "\n",
    "# Document frequency (df)\n",
    "df = defaultdict(int)\n",
    "for d in train:\n",
    "    r = ''.join([c for c in d['review_text'].lower() if not c in punctuation])\n",
    "    for w in set(r.split()):\n",
    "        df[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7016208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term frequency (tf)\n",
    "# Here we extract frequencies for terms in a single specific review\n",
    "# the first review in the dataset\n",
    "rev = train[0] # Query review\n",
    "rev\n",
    "\n",
    "tf = defaultdict(int)\n",
    "r = ''.join([c for c in rev['review_text'].lower() if not c in punctuation])\n",
    "for w in r.split():\n",
    "    # Note = rather than +=, different versions of tf could be used instead\n",
    "    tf[w] = 1\n",
    "    \n",
    "tfidf = dict(zip(words,[tf[w] * math.log2(len(train) / df[w]) for w in words]))\n",
    "tfidfQuery = [tf[w] * math.log2(len(train) / df[w]) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5aeaae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7.356975041986563, 'vampire'),\n",
       " (6.930160374931366, 'crime'),\n",
       " (6.748553568441418, 'popular'),\n",
       " (6.687799537362322, 'nature'),\n",
       " (6.65835575946984, 'horrible'),\n",
       " (6.546245393148302, 'create'),\n",
       " (6.5328248773859805, 'date'),\n",
       " (6.50635266602479, 'strength'),\n",
       " (6.50635266602479, 'created'),\n",
       " (6.368849142274855, 'wouldnt')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the highest tf-idf words in our example review\n",
    "maxTf = [(tf[w],w) for w in words]\n",
    "maxTf.sort(reverse=True)\n",
    "maxTfIdf = [(tfidf[w],w) for w in words]\n",
    "maxTfIdf.sort(reverse=True)\n",
    "\n",
    "maxTfIdf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fd0ccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity\n",
    "def Cosine(x1,x2):\n",
    "    numer = 0\n",
    "    norm1 = 0\n",
    "    norm2 = 0\n",
    "    for a1,a2 in zip(x1,x2):\n",
    "        numer += a1*a2\n",
    "        norm1 += a1**2\n",
    "        norm2 += a2**2\n",
    "    if norm1*norm2:\n",
    "        return numer / math.sqrt(norm1*norm2)\n",
    "    return 0\n",
    "\n",
    "# Find the other reviews in the corpus with the highest cosine similarity between tf-idf vectors\n",
    "similarities = []\n",
    "for rev2 in train:\n",
    "    tf = defaultdict(int)\n",
    "    r = ''.join([c for c in rev2['review_text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        # Note = rather than +=\n",
    "        tf[w] = 1\n",
    "    tfidf2 = [tf[w] * math.log2(len(train) / df[w]) for w in words]\n",
    "    similarities.append((Cosine(tfidfQuery, tfidf2), rev2['review_id'], rev2['review_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "895e0178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0,\n",
       "  '66b2ba840f9bd36d6d27f46136fe4772',\n",
       "  'Sherlock Holmes and the Vampires of London \\n Release Date: April 2014 \\n Publisher: Darkhorse Comics \\n Story by: Sylvain Cordurie \\n Art by: Laci \\n Colors by: Axel Gonzabo \\n Cover by: Jean Sebastien Rossbach \\n ISDN: 9781616552664 \\n MSRP: $17.99 Hardcover \\n \"Sherlock Holmes died fighting Professor Moriarty in the Reichenbach Falls. \\n At least, that\\'s what the press claims. \\n However, Holmes is alive and well and taking advantage of his presumed death to travel the globe. \\n Unfortunately, Holmes\\'s plans are thwarted when a plague of vampirism haunts Britain. \\n This book collects Sherlock Holmes and the Vampires of London Volumes 1 and 2, originally created by French publisher Soleil.\" - Darkhorse Comics \\n When I received this copy of \"Sherlock Holmes and the Vampires of London\" I was Ecstatic! The cover art was awesome and it was about two of my favorite things, Sherlock Holmes and Vampires. I couldn\\'t wait to dive into this! \\n Unfortunately, that is where my excitement ended. The story takes place a month after Sherlock Holmes supposed death in his battle with Professor Moriarty. Sherlock\\'s plan to stay hidden and out of site are ruined when on a trip with his brother Mycroft, they stumble on the presence of vampires. That is about as much of Sherlock\\'s character that comes through the book. I can\\'t even tell you the story really because nothing and I mean nothing stuck with me after reading it. I never, ever got the sense of Sherlock Holmes anywhere in this graphic novel, nor any real sense of mystery or crime. It was just Sherlock somehow battling vampires that should have had absolutely no trouble snuffing him out in a fight, but somehow always surviving and holding his own against supernatural, super powerful, blazingly fast creatures. \\n The cover art is awesome and it truly made me excited to read this but everything else feel completely flat for me. I tried telling myself that \"it\\'s a graphic novel, it would be hard to translate mystery, details, emotion\" but then I remembered reading DC Comic\\'s \"Identity Crisis\" and realized that was a load of crap. I know it\\'s unfair to compare the two as \"Identity Crisis\" had popular mystery author Brad Meltzer writing it right? Yeah....no. The standard was set that day and there is more than enough talent out there to create a great story in a graphic novel. \\n That being said, it wasn\\'t a horrible story, it just didn\\'t grip me for feel anything like Sherlock Holmes to me. It was easy enough to follow but I felt no sense of tension, stakes or compassion for any of the characters. \\n As far as the vampires go, it\\'s hard to know what to expect anymore as there are so many different versions these days. This was the more classic version which I personally prefer, but again I didn\\'t find anything that portrayed their dominance, calm confidence or sexuality. There was definitely a presence of their physical prowess but somehow that was lost on me as easily as Sherlock was able to defend himself. I know it, wouldn\\'t do to kill of the main character, but this would have a been a great opportunity to build around the experience and beguiling nature of a vampire that had lived so many years of experience. Another chance to showcase Sherlock\\'s intellect in a battle of wits over strength in something more suitable for this sort of story as apposed to trying to make it feel like an action movie. \\n Maybe I expected to much and hoped to have at least a gripping premise or some sort of interesting plot or mystery but I didn\\'t find it here. This may be a must have for serious Sherlock Holmes fans that have to collect everything about him, but if you are looking for a great story inside a graphic novel, I would have to say pass on this one. \\n That artwork is good, cover is great, story is lacking so I am giving it 2.5 out of 5 stars.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities.sort(reverse=True)\n",
    "similarities[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302347ff",
   "metadata": {},
   "source": [
    "# Content, Structure, and Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a6c712",
   "metadata": {},
   "source": [
    "For these tasks, consider the complete dataset, but discard any users who have fewer than three interactions. Build a training set by considering all but the last interaction from each user (in order of timestamp). The test set will then contain each userâs most recent interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9856d7",
   "metadata": {},
   "source": [
    "Q3.Using the fastFM library1 compare three model variants to predict ratings: (adapting the FPMC Tensorflow code from Chapter 7, if using the Tensorflow code you can report and estimate the AUC by measuring how often the âpredictâ function returns a positive score (positive item has a higher score than the negative item).)\n",
    "\n",
    "(a) A regular latent-factor model, i.e., one which includes a user term and an item term (f(u,i));\n",
    "(b) A non-personalized Markov Chain model, i.e., one which combines a one-hot encoding of the previous item with a one-hot encoding of the current item (f(i,i(prev))).\n",
    "(c) A model which includes both the user, the item, and the previous item (f(u,i,i(prev))).\n",
    "\n",
    "Report the training and test MSE for each variant; you may follow the same hyperparameter settings as in the starter code (3 marks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe8415d",
   "metadata": {},
   "source": [
    "I used  the Tensorflow code implements a Bayesian Personalized Ranking-style model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcbbb1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil\n",
    "from collections import defaultdict\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec297fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseData(fname):\n",
    "    for l in open(fname):\n",
    "        yield eval(l)\n",
    "\n",
    "userIDs = {}\n",
    "itemIDs = {}\n",
    "interactions = []\n",
    "interactionsPerUser = defaultdict(list)\n",
    "itemsPerUser = defaultdict(list)\n",
    "\n",
    "for d in parseData(\"goodreads_reviews_comics_graphic.json\"):\n",
    "    u = d['user_id']\n",
    "    i = d['book_id']\n",
    "    t = d['date_added']\n",
    "    r = d['rating']\n",
    "    dt = dateutil.parser.parse(t)\n",
    "    t = int(dt.timestamp())\n",
    "    itemsPerUser[u].append((i,t,r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "194dcf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = itemsPerUser.keys()\n",
    "for u in user_id:\n",
    "    if len(itemsPerUser[u]) >= 3: # discard any users who have fewer than three interactions\n",
    "        itemsPerUser[u][0][0]\n",
    "        for d in range(len(itemsPerUser[u])):\n",
    "            i = itemsPerUser[u][d][0]\n",
    "            t = itemsPerUser[u][d][1]\n",
    "            r = itemsPerUser[u][d][2]\n",
    "            if not u in userIDs: userIDs[u] = len(userIDs)\n",
    "            if not i in itemIDs: itemIDs[i] = len(itemIDs)\n",
    "            interactions.append((t,u,i,r))\n",
    "            interactionsPerUser[u].append((t,i,r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9ae1b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1470834408, 'bafc2d50014200cda7cb2b6acd60cd73', '6315584', 4)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interaction with timestamp\n",
    "interactions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea3165be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "498312"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba3bf91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions.sort()\n",
    "itemIDs['dummy'] = len(itemIDs)\n",
    "# Build a data structure including users, items, and their previous items\n",
    "interactionsWithPrevious = []\n",
    "\n",
    "for u in interactionsPerUser:\n",
    "    interactionsPerUser[u].sort()\n",
    "    lastItem = 'dummy'\n",
    "    for (t,i,r) in interactionsPerUser[u]:\n",
    "        interactionsWithPrevious.append((t,u,i,lastItem,r))\n",
    "        lastItem = i\n",
    "\n",
    "itemsPerUser = defaultdict(set)\n",
    "for _,u,i,_ in interactions:\n",
    "    itemsPerUser[u].add(i)\n",
    "    \n",
    "items = list(itemIDs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7bd55ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train and test\n",
    "# sort by user and time\n",
    "interactionsWithPrevious = sorted(interactionsWithPrevious, key=lambda element: (element[1], element[0]))\n",
    "interactionsTrain = []\n",
    "interactionsTest = []\n",
    "# take the last interaction of each user as the test\n",
    "for n in range(1,len(interactionsWithPrevious)):\n",
    "    if interactionsWithPrevious[n][3] == 'dummy':\n",
    "        test = interactionsWithPrevious[n-1]\n",
    "        interactionsTest.append(test)\n",
    "    else:\n",
    "        train = interactionsWithPrevious[n-1]\n",
    "        if train[3] != 'dummy':\n",
    "            interactionsTrain.append(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc6b5412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tensorflow model. \n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(0.1)\n",
    "\n",
    "class FPMC(tf.keras.Model):\n",
    "    def __init__(self, K, lamb, UI = 1, IJ = 1):\n",
    "        super(FPMC, self).__init__()\n",
    "        # Initialize variables\n",
    "        self.betaI = tf.Variable(tf.random.normal([len(itemIDs)],stddev=0.001))\n",
    "        self.gammaUI = tf.Variable(tf.random.normal([len(userIDs),K],stddev=0.001))\n",
    "        self.gammaIU = tf.Variable(tf.random.normal([len(itemIDs),K],stddev=0.001))\n",
    "        self.gammaIJ = tf.Variable(tf.random.normal([len(itemIDs),K],stddev=0.001))\n",
    "        self.gammaJI = tf.Variable(tf.random.normal([len(itemIDs),K],stddev=0.001))\n",
    "        # Regularization coefficient\n",
    "        self.lamb = lamb\n",
    "        # Which terms to include\n",
    "        self.UI = UI\n",
    "        self.IJ = IJ\n",
    "\n",
    "    # Prediction for a single instance\n",
    "    def predict(self, u, i, j):\n",
    "        p = self.betaI[i] + self.UI * tf.tensordot(self.gammaUI[u], self.gammaIU[i], 1) +\\\n",
    "                            self.IJ * tf.tensordot(self.gammaIJ[i], self.gammaJI[j], 1)\n",
    "        return p\n",
    "\n",
    "    # Regularizer\n",
    "    def reg(self):\n",
    "        return self.lamb * (tf.nn.l2_loss(self.betaI) +\\\n",
    "                            tf.nn.l2_loss(self.gammaUI) +\\\n",
    "                            tf.nn.l2_loss(self.gammaIU) +\\\n",
    "                            tf.nn.l2_loss(self.gammaIJ) +\\\n",
    "                            tf.nn.l2_loss(self.gammaJI))\n",
    "\n",
    "    def call(self, sampleU, # user\n",
    "                   sampleI, # item\n",
    "                   sampleJ, # previous item\n",
    "                   sampleK): # negative item\n",
    "        u = tf.convert_to_tensor(sampleU, dtype=tf.int32)\n",
    "        i = tf.convert_to_tensor(sampleI, dtype=tf.int32)\n",
    "        j = tf.convert_to_tensor(sampleJ, dtype=tf.int32)\n",
    "        k = tf.convert_to_tensor(sampleK, dtype=tf.int32)\n",
    "        gamma_ui = tf.nn.embedding_lookup(self.gammaUI, u)\n",
    "        gamma_iu = tf.nn.embedding_lookup(self.gammaIU, i)\n",
    "        gamma_ij = tf.nn.embedding_lookup(self.gammaIJ, i)\n",
    "        gamma_ji = tf.nn.embedding_lookup(self.gammaJI, j)\n",
    "        beta_i = tf.nn.embedding_lookup(self.betaI, i)\n",
    "        x_uij = beta_i + self.UI * tf.reduce_sum(tf.multiply(gamma_ui, gamma_iu), 1) +\\\n",
    "                         self.IJ * tf.reduce_sum(tf.multiply(gamma_ij, gamma_ji), 1)\n",
    "        gamma_uk = tf.nn.embedding_lookup(self.gammaUI, u)\n",
    "        gamma_ku = tf.nn.embedding_lookup(self.gammaIU, k)\n",
    "        gamma_kj = tf.nn.embedding_lookup(self.gammaIJ, k)\n",
    "        gamma_jk = tf.nn.embedding_lookup(self.gammaJI, j)\n",
    "        beta_k = tf.nn.embedding_lookup(self.betaI, k)\n",
    "        x_ukj = beta_k + self.UI * tf.reduce_sum(tf.multiply(gamma_uk, gamma_ku), 1) +\\\n",
    "                         self.IJ * tf.reduce_sum(tf.multiply(gamma_kj, gamma_jk), 1)\n",
    "        return -tf.reduce_mean(tf.math.log(tf.math.sigmoid(x_uij - x_ukj)))\n",
    "\n",
    "\n",
    "\n",
    "def trainingStep(model, interactions):\n",
    "    with tf.GradientTape() as tape:\n",
    "        sampleU, sampleI, sampleJ, sampleK = [], [], [], []\n",
    "        for _ in range(100000):\n",
    "            _,u,i,j,_ = random.choice(interactions) # positive sample\n",
    "            k = random.choice(items) # negative sample\n",
    "            while k in itemsPerUser[u]:\n",
    "                k = random.choice(items)\n",
    "            sampleU.append(userIDs[u])\n",
    "            sampleI.append(itemIDs[i])\n",
    "            sampleJ.append(itemIDs[j])\n",
    "            sampleK.append(itemIDs[k])\n",
    "\n",
    "        loss = model(sampleU,sampleI,sampleJ,sampleK)\n",
    "        loss += model.reg()\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients((grad, var) for\n",
    "                              (grad, var) in zip(gradients, model.trainable_variables)\n",
    "                              if grad is not None)\n",
    "    return loss.numpy()\n",
    "\n",
    "\n",
    "modelFPMC = FPMC(5, 0.001, 1, 1) # user, item, previous item\n",
    "modelMF = FPMC(5, 0.001, 1, 0) # user, item\n",
    "modelFMC = FPMC(5, 0.001, 0, 1) # item, previous item\n",
    "\n",
    "\n",
    "interactionsTestPerUser = defaultdict(set)\n",
    "itemSet = set()\n",
    "for _,u,i,j,_ in interactionsTest:\n",
    "    interactionsTestPerUser[u].add((i,j))\n",
    "    itemSet.add(i)\n",
    "    itemSet.add(j)\n",
    "\n",
    "def AUCu(model, u, N):\n",
    "    win = 0\n",
    "    if N > len(interactionsTestPerUser[u]):\n",
    "        N = len(interactionsTestPerUser[u])\n",
    "    positive = random.sample(interactionsTestPerUser[u],N)\n",
    "    negative = random.sample(itemSet,N)\n",
    "    for (i,j),k in zip(positive,negative):\n",
    "        sp = model.predict(userIDs[u], itemIDs[i], itemIDs[j]).numpy()\n",
    "        sn = model.predict(userIDs[u], itemIDs[k], itemIDs[j]).numpy()\n",
    "        if sp > sn:\n",
    "            win += 1\n",
    "    return win/N\n",
    "\n",
    "def AUC(model):\n",
    "    av = []\n",
    "    for u in interactionsTestPerUser:\n",
    "        av.append(AUCu(model, u, 10))\n",
    "    return sum(av) / len(av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18cdfbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 10, objective = 0.8851714\n",
      "iteration 20, objective = 0.79832035\n",
      "iteration 30, objective = 0.6918548\n",
      "iteration 40, objective = 0.6882886\n",
      "iteration 50, objective = 0.6807407\n",
      "iteration 60, objective = 0.6786849\n",
      "iteration 70, objective = 0.6778456\n",
      "iteration 80, objective = 0.6770646\n",
      "iteration 90, objective = 0.67785627\n",
      "iteration 100, objective = 0.6778191\n",
      "FPMC AUC = 0.7089346655153982\n",
      "iteration 10, objective = 1.117121\n",
      "iteration 20, objective = 0.90041363\n",
      "iteration 30, objective = 0.7413565\n",
      "iteration 40, objective = 0.6986629\n",
      "iteration 50, objective = 0.687025\n",
      "iteration 60, objective = 0.68199456\n",
      "iteration 70, objective = 0.68072826\n",
      "iteration 80, objective = 0.6794847\n",
      "iteration 90, objective = 0.6786478\n",
      "iteration 100, objective = 0.6791993\n",
      "MF AUC = 0.7016981209654208\n",
      "iteration 10, objective = 1.1579998\n",
      "iteration 20, objective = 0.9019433\n",
      "iteration 30, objective = 0.8522896\n",
      "iteration 40, objective = 0.75185883\n",
      "iteration 50, objective = 0.69575876\n",
      "iteration 60, objective = 0.6846295\n",
      "iteration 70, objective = 0.6816081\n",
      "iteration 80, objective = 0.6796422\n",
      "iteration 90, objective = 0.6794811\n",
      "iteration 100, objective = 0.67915744\n",
      "FMC AUC = 0.707002179186711\n"
     ]
    }
   ],
   "source": [
    "for model,name in [(modelFPMC,\"FPMC\"), (modelMF,\"MF\"), (modelFMC,\"FMC\")]:\n",
    "    for i in range(100):\n",
    "        obj = trainingStep(model, interactionsTrain)\n",
    "        if (i % 10 == 9): print(\"iteration \" + str(i+1) + \", objective = \" + str(obj))\n",
    "    print(name + \" AUC = \" + str(AUC(model)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19f4c8e",
   "metadata": {},
   "source": [
    "From the AUC results, we can tell that the FPMC > FMC > MF model. The FPMC model works best for the binary prediction of items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3c4826",
   "metadata": {},
   "source": [
    "Q4. Experiment with the factorization machine code to see whether performance of the above models can be improved by incorporating any other features from the data (1 mark).4 If you canât get any FM library working, you may skip this question and simply describe what features you might try."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5911976",
   "metadata": {},
   "source": [
    "I might try to:\n",
    "\n",
    "1) include text content to predict ratings.\n",
    "From Taks 1, I already get some most poitive and negative unigrams and bigrams. I can assign each review_text with a positive ranking and predict the ratings.\n",
    "\n",
    "2) include previous ratings.\n",
    "Previous rating might have prediction power in terms of evaluating the user's rating standard. Some users in general tend to give higher/lower ratings than other."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
